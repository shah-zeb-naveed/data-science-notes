{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQQdZCFlWhLfgPyscEW1zV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shah-zeb-naveed/data-science-notes/blob/main/ml_algos_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implement cosine similarity between two vectors.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "  \"\"\"Calculates the cosine similarity between two vectors.\n",
        "\n",
        "  Args:\n",
        "    vec1: The first vector.\n",
        "    vec2: The second vector.\n",
        "\n",
        "  Returns:\n",
        "    The cosine similarity between the two vectors.\n",
        "  \"\"\"\n",
        "  dot_product = np.dot(vec1, vec2)\n",
        "  magnitude_vec1 = np.linalg.norm(vec1)\n",
        "  magnitude_vec2 = np.linalg.norm(vec2)\n",
        "  if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
        "    return 0  # Handle cases where either vector has zero magnitude.\n",
        "  return dot_product / (magnitude_vec1 * magnitude_vec2)\n",
        "\n",
        "vec1 = [1, 5]\n",
        "vec2 = [3, 6]\n",
        "cosine_similarity(vec1, vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wxEDMX6v9Va",
        "outputId": "0f8a684f-4845-4c95-db46-fabc2fc194ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9647638212377322)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean(vec1, vec2):\n",
        "  \"\"\"Calculates the Euclidean distance between two vectors.\n",
        "\n",
        "  Args:\n",
        "    vec1: The first vector.\n",
        "    vec2: The second vector.\n",
        "\n",
        "  Returns:\n",
        "    The Euclidean distance between the two vectors.\n",
        "  \"\"\"\n",
        "  # replace for loop with vectorization\n",
        "  return np.sqrt(np.sum((vec1 - vec2) ** 2))\n",
        "\n",
        "vec1 = np.array([1, 5])\n",
        "vec2 = np.array([3, 6])\n",
        "euclidean(vec1, vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKiZTMdDwYLS",
        "outputId": "3c320980-4d2e-452e-c243-da58abc738c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(2.23606797749979)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create function that calculates softmax\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Calculates the softmax of a vector.\n",
        "\n",
        "    Args:\n",
        "      x: A NumPy array or list representing the input vector.\n",
        "\n",
        "    Returns:\n",
        "      A NumPy array representing the softmax of the input vector.\n",
        "    \"\"\"\n",
        "    e_x = np.exp(x) # - np.max(x))  # Subtract max for numerical stability\n",
        "    return e_x / np.sum(np.exp(x))\n",
        "\n",
        "x = np.array([1, 2, 3])\n",
        "softmax(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53RuyaSuw8LJ",
        "outputId": "543c222e-fe00-43c6-b90c-7dcf32792b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09003057, 0.24472847, 0.66524096])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gini impurity and entropy calculators.\n",
        "\n",
        "def gini_at_node(proportions):\n",
        "  \"\"\"Calculates the Gini impurity at a node given class proportions.\n",
        "\n",
        "  Args:\n",
        "    proportions: A list of proportions of each class at the node.\n",
        "                 The proportions should sum to 1.\n",
        "\n",
        "  Returns:\n",
        "    The Gini impurity at the node.\n",
        "  \"\"\"\n",
        "  gini = 1 - np.sum(np.square(proportions))\n",
        "  return gini\n",
        "\n",
        "def entropy_at_node(proportions):\n",
        "  \"\"\"Calculates the entropy at a node given class proportions.\n",
        "\n",
        "  Args:\n",
        "    proportions: A list of proportions of each class at the node.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  entropy = -np.sum(proportions * np.log2(proportions))\n",
        "  return entropy\n",
        "\n",
        "proportions = [0.5, 0.5]\n",
        "gini_at_node(proportions), entropy_at_node(proportions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvNCgQnyxp2S",
        "outputId": "b00c1371-f947-415c-f8a7-7e9e7825297e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.5), np.float64(1.0))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def split_gini_gain(left_child, right_child):\n",
        "    n_left, n_right = len(left_child), len(right_child)\n",
        "    n_total = n_left + n_right\n",
        "\n",
        "    def gini(node):\n",
        "        if len(node) == 0:\n",
        "            return 0.0\n",
        "        labels, counts = np.unique(node, return_counts=True)\n",
        "        probs = counts / counts.sum()\n",
        "        return 1.0 - np.sum(probs**2)\n",
        "\n",
        "    gini_left = gini(left_child)\n",
        "    gini_right = gini(right_child)\n",
        "\n",
        "    return (n_left / n_total) * gini_left + (n_right / n_total) * gini_right\n",
        "\n",
        "left_child = np.array([0, 0, 0, 0, 0, 1, 1])\n",
        "right_child = np.array([1, 1, 1, 1, 1, 0, 0])\n",
        "\n",
        "gini_impurity_value = split_gini_gain(left_child, right_child)\n",
        "gini_impurity_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc36Z1mzzFLl",
        "outputId": "a8315544-57ac-49f8-aaee-7ccbd7df6603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.40816326530612246)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate code for # KNN (including distance computation)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def euclidean_distance(x1, x2):\n",
        "    return np.sqrt(np.sum((x1-x2)**2))\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # lazy learner. nothing computed here.\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # compute distances\n",
        "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # get k nearest samples, labels\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "\n",
        "        # majority vote, most common class label\n",
        "        most_common = np.argmax(np.bincount(k_nearest_labels))\n",
        "\n",
        "        # if regression, return np.mean(k_nearest_labels)\n",
        "        return most_common\n",
        "\n",
        "example_X = np.array([[6, 11], [25, 24], [3, 4], [4, 5]])\n",
        "example_y = np.array([0, 0, 1, 1])\n",
        "\n",
        "knn = KNN(k=3)\n",
        "knn.fit(example_X, example_y)\n",
        "knn.predict(np.array([[2, 2]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU5IAlG03xtw",
        "outputId": "32c41509-a0ea-4844-b2b9-c7847290662c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Linear transformation\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        # Sigmoid activation (probabilities)\n",
        "        predictions = self.sigmoid(z)\n",
        "        return predictions\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        self.weights = np.zeros(num_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Forward pass\n",
        "            predictions = self.forward(X)\n",
        "\n",
        "            # Compute gradients via chain rule\n",
        "            # log loss simplifies to\n",
        "            dw = (1 / num_samples) * np.dot(X.T, (predictions - y))\n",
        "            db = (1 / num_samples) * np.sum(predictions - y)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights += self.learning_rate * (-dw)\n",
        "            self.bias += self.learning_rate * (-db)\n",
        "\n",
        "# Example input\n",
        "X_train = np.array([[1, 2], [2, 3], [3, 4]])  # 3 samples, 2 features\n",
        "y_train = np.array([0, 1, 0])  # Target labels\n",
        "\n",
        "# Create and fit the model\n",
        "model = LogisticRegression(learning_rate=0.01, epochs=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions for new data\n",
        "predictions = model.forward(np.array([[1, 1]]))  # Forward pass with new sample\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRPh7fH56U7v",
        "outputId": "f2b881c5-b7ae-443a-bb9f-096103cc4d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0.42013584]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLFIn05LMMkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a function for cross-entropy loss\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    \"\"\"Calculates the cross-entropy loss between true and predicted labels.\n",
        "\n",
        "    Args:\n",
        "        y_true: A NumPy array of true labels (one-hot encoded).\n",
        "        y_pred: A NumPy array of predicted probabilities.\n",
        "\n",
        "    Returns:\n",
        "        The cross-entropy loss.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-15  # Small value to avoid log(0) errors\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon) #numerical stability. for 0 and 1\n",
        "    loss = -np.sum(y_true * np.log(y_pred)) / len(y_true)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "nLs8hnB5Pi_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create func for MSE loss\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    \"\"\"Calculates the mean squared error (MSE) loss.\n",
        "\n",
        "    Args:\n",
        "        y_true: A NumPy array of true labels.\n",
        "        y_pred: A NumPy array of predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        The MSE loss.\n",
        "    \"\"\"\n",
        "    return np.mean((y_true - y_pred)**2)\n"
      ],
      "metadata": {
        "id": "KlkRWBBt0dx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fM-bCaYptwIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Decision tree node splitting (Gini impurity or entropy)."
      ],
      "metadata": {
        "id": "EdEvVtp-tjEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: A function to find best split in DT\n",
        "\n",
        "import numpy as np\n",
        "def best_split(X, y):\n",
        "    \"\"\"Finds the best split for a decision tree node.\n",
        "\n",
        "    Args:\n",
        "        X: The feature matrix.\n",
        "        y: The target vector.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the best split feature index, the best split threshold,\n",
        "        and the minimum Gini impurity.\n",
        "    \"\"\"\n",
        "    best_feature_index = None\n",
        "    best_threshold = None\n",
        "    min_gini_impurity = 1.0  # Initialize with the maximum possible value\n",
        "\n",
        "    num_features = X.shape[1]\n",
        "    for feature_index in range(num_features):\n",
        "        thresholds = np.unique(X[:, feature_index])\n",
        "        for threshold in thresholds:\n",
        "            left_child_indices = X[:, feature_index] <= threshold\n",
        "            right_child_indices = X[:, feature_index] > threshold\n",
        "            left_child_labels = y[left_child_indices]\n",
        "            right_child_labels = y[right_child_indices]\n",
        "            gini_impurity_gain = split_gini_gain(left_child_labels, right_child_labels)\n",
        "\n",
        "            if gini_impurity_gain < min_gini_impurity:\n",
        "                min_gini_impurity = gini_impurity_gain\n",
        "                best_feature_index = feature_index\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature_index, best_threshold, min_gini_impurity\n",
        "\n",
        "# example\n",
        "\n",
        "# data for current split\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
        "y = np.array([0, 0, 1, 1])\n",
        "\n",
        "best_feature_index, best_threshold, min_gini_impurity = best_split(X, y)\n",
        "print(best_feature_index, best_threshold, min_gini_impurity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QutwYBgCtdxk",
        "outputId": "445c4aa0-8708-47f9-f45d-f0a5090d0f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Linear Regression class with batch and mini-bastch gradient descent, with regularization option\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000, regularization=None, lambda_param=0.1):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.regularization = regularization\n",
        "        self.lambda_param = lambda_param\n",
        "\n",
        "    def _run_batch_gradient_descent(self, X, y):\n",
        "      n_samples = X.shape[0]\n",
        "      y_predicted = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "      # chain rule\n",
        "      dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "      db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "      # Regularization\n",
        "      if self.regularization == 'l1':\n",
        "        dw += (self.lambda_param / n_samples) * np.sign(self.weights)\n",
        "      elif self.regularization == 'l2':\n",
        "        dw += (2 * self.lambda_param / n_samples) * self.weights\n",
        "\n",
        "      self.weights -= self.learning_rate * dw\n",
        "      self.bias -= self.learning_rate * db\n",
        "\n",
        "    def fit(self, X, y, batch_size=None):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # init parameters\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # train model\n",
        "        for _ in range(self.n_iters):\n",
        "          if batch_size is None: # Batch gradient descent\n",
        "            for _ in range(self.n_iters):\n",
        "                self._run_batch_gradient_descent(X, y)\n",
        "          else: # Mini-batch gradient descent\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                self._run_batch_gradient_descent(X_batch, y_batch)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_approximated = np.dot(X, self.weights) + self.bias\n",
        "        return y_approximated\n",
        "\n",
        "# example\n",
        "\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
        "y = np.array([1, 2, 3, 4])\n",
        "reg = LinearRegression(learning_rate=0.01, n_iters=1000, regularization='l1', lambda_param=0.1)\n",
        "reg.fit(X, y, batch_size=2)\n",
        "print(reg.predict(X))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrC3DPWZwiJC",
        "outputId": "00ce2ef3-c901-42c8-c908-6834d78caa2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.06721502 2.02743833 2.98766165 3.94788496]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: implement a simple KFold class\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class KFoldMy:\n",
        "    def __init__(self, n_splits=5):\n",
        "        self.n_splits = n_splits\n",
        "\n",
        "    def split(self, X, y=None):\n",
        "        n_samples = len(X)\n",
        "        fold_size = n_samples // self.n_splits\n",
        "        indices = np.arange(n_samples)\n",
        "\n",
        "        print('fold_size', fold_size)\n",
        "        print('n_samples', n_samples)\n",
        "\n",
        "\n",
        "        for i in range(self.n_splits):\n",
        "            start = i * fold_size\n",
        "            end = (i + 1) * fold_size\n",
        "            #end = start + fold_size\n",
        "            test_indices = indices[start:end]\n",
        "            train_indices = np.concatenate([indices[:start], indices[end:]])\n",
        "            print(len(train_indices), len(test_indices))\n",
        "            print(train_indices, test_indices)\n",
        "            yield train_indices, test_indices"
      ],
      "metadata": {
        "id": "9mjGbFZL4OkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: K-fold cross validation\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def k_fold_cross_validation(model, X, y, k=5):\n",
        "    kf = KFoldMy(n_splits=k) # , shuffle=True, random_state=42\n",
        "    scores = []\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_test)\n",
        "        score = mse_loss(y_test, predictions)\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5,6],[6,7], [2, 3], [3, 4], [4, 5], [5,6],[6,7]])\n",
        "y = np.array([1, 2, 3, 4, 5, 6, 2, 3, 4, 5, 6])\n",
        "\n",
        "print('input:', len(X))\n",
        "reg = LinearRegression()\n",
        "mean_score, std_score = k_fold_cross_validation(reg, X, y, k=3)\n",
        "print(f\"Mean loss: {mean_score:.4f}, Standard deviation: {std_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVtO4CDyzWG8",
        "outputId": "59fee471-7d37-4010-e2c4-7d31d2f6b8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: 11\n",
            "fold_size 3\n",
            "n_samples 11\n",
            "8 3\n",
            "[ 3  4  5  6  7  8  9 10] [0 1 2]\n",
            "8 3\n",
            "[ 0  1  2  6  7  8  9 10] [3 4 5]\n",
            "8 3\n",
            "[ 0  1  2  3  4  5  9 10] [6 7 8]\n",
            "Mean loss: 0.0000, Standard deviation: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate func for stratified sampling\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def stratified_sampling(df, column, num_samples):\n",
        "    \"\"\"\n",
        "    Performs stratified sampling on a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "      df: The input DataFrame.\n",
        "      column: The column to stratify by.\n",
        "      num_samples: The total number of samples to draw.\n",
        "\n",
        "    Returns:\n",
        "      A new DataFrame with the stratified sample.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate proportions for each stratum\n",
        "    proportions = df[column].value_counts(normalize=True)\n",
        "\n",
        "    # Calculate the number of samples for each stratum\n",
        "    samples_per_stratum = (proportions * num_samples).astype(int)\n",
        "    return samples_per_stratum\n",
        "\n",
        "    # Adjust sample sizes to match total num_samples (due to rounding)\n",
        "    diff = num_samples - samples_per_stratum.sum()\n",
        "    if diff > 0 :\n",
        "        samples_per_stratum[samples_per_stratum.index[0]] += diff\n",
        "\n",
        "    # Sample from each stratum\n",
        "    stratified_sample = []\n",
        "    for stratum, num_samples_stratum in samples_per_stratum.items():\n",
        "      stratified_sample.append(df[df[column] == stratum].sample(n=num_samples_stratum, random_state=42))\n",
        "\n",
        "    # Concatenate the samples\n",
        "    return pd.concat(stratified_sample)\n"
      ],
      "metadata": {
        "id": "jZxBQmPJ_M1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "si1Yhe1DBiXc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}