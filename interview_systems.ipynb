{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUF1ICoP8yCRafxZoFWPPY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shah-zeb-naveed/data-science-notes/blob/main/interview_systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile lru_cache.py\n",
        "import sys\n",
        "import requests\n",
        "from collections import OrderedDict\n",
        "\n",
        "class ImageCache:\n",
        "    def __init__(self, max_size_bytes):\n",
        "        self.max_size = max_size_bytes\n",
        "        self.cache = OrderedDict()\n",
        "        self.current_size = 0\n",
        "\n",
        "    def get(self, url):\n",
        "        if url in self.cache:\n",
        "            # Move to end to mark as recently used\n",
        "            self.cache.move_to_end(url)\n",
        "            return f\"Cache Hit: {url}\"\n",
        "        else:\n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                response.raise_for_status()\n",
        "                image_bytes = response.content\n",
        "                image_size = len(image_bytes)\n",
        "\n",
        "                # If image is too big to cache, skip caching\n",
        "                if image_size > self.max_size:\n",
        "                    return f\"Too big. Downloaded (Not Cached): {url}\"\n",
        "\n",
        "                # Evict least recently used items until there's space\n",
        "                while self.current_size + image_size > self.max_size and self.cache:\n",
        "                    _, old_bytes = self.cache.popitem(last=False) # first added\n",
        "                    self.current_size -= len(old_bytes)\n",
        "\n",
        "                self.cache[url] = image_bytes\n",
        "                self.current_size += image_size\n",
        "                return f\"Downloaded and Cached: {url}\"\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                return f\"Error downloading image: {e}\"\n",
        "\n",
        "def main():\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python script.py <input_file>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # read fome file\n",
        "    # input_file = sys.argv[1]\n",
        "    # with open(input_file, 'r') as f:\n",
        "    #     lines = f.read().splitlines()\n",
        "\n",
        "    # max_size = int(lines[0])\n",
        "    # num_urls = int(lines[1])\n",
        "    # urls = lines[2:]\n",
        "\n",
        "    # read from command line\n",
        "    max_size = int(sys.argv[1])\n",
        "    num_urls = int(sys.argv[2])\n",
        "    urls = sys.argv[3:]\n",
        "\n",
        "    cache = ImageCache(max_size)\n",
        "\n",
        "    for url in urls:\n",
        "        result = cache.get(url)\n",
        "        print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aZ72Rp4-10C",
        "outputId": "e08ec68f-4697-4414-b11a-3d634809fada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting lru_cache.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python lru_cache.py 10000 3 https://placehold.co/600x400.png https://placehold.co/600x400.png https://placehold.co/600x400.png"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzmeuD_u_zy_",
        "outputId": "4cf182a0-2239-46d6-a4fd-83cd3347db97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded and Cached: https://placehold.co/600x400.png\n",
            "Cache Hit: https://placehold.co/600x400.png\n",
            "Cache Hit: https://placehold.co/600x400.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min({2:'A',1:'B'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBCZEJvAKI7s",
        "outputId": "2b749de5-f8ae-4b51-da1a-aa679e30a3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile advanced_lru_cache.py\n",
        "import os\n",
        "import sys\n",
        "import requests\n",
        "#import threading\n",
        "import time\n",
        "import pickle\n",
        "from collections import OrderedDict, defaultdict, deque\n",
        "from urllib.parse import urlparse\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from heapq import heappush, heappop\n",
        "\n",
        "\n",
        "\n",
        "class CacheItem:\n",
        "    def __init__(self, url, content, size, timestamp, priority=0):\n",
        "        self.url = url\n",
        "        self.content = content\n",
        "        self.size = size\n",
        "        self.last_access = timestamp\n",
        "        self.access_count = 1\n",
        "        self.insert_time = timestamp\n",
        "        self.priority = priority\n",
        "        self.ttl_expiry = timestamp + 300  # default TTL of 5 minutes\n",
        "        self.lfu_heap = []  # minheap for LFU\n",
        "        self.fifo_queue = deque()  # for FIFO\n",
        "\n",
        "\n",
        "class ImageCache:\n",
        "    def __init__(self, max_size_bytes, eviction_policy=\"FIFO\", max_item_size=10_000_000, bandwidth_limit=5, snapshot_file=\"cache_snapshot.pkl\"):\n",
        "        self.max_size = max_size_bytes\n",
        "        self.max_item_size = max_item_size\n",
        "\n",
        "        self.size = 0\n",
        "#        self.lock = threading.RLock()\n",
        "        self.eviction_policy = eviction_policy\n",
        "\n",
        "        self.bandwidth_limit = bandwidth_limit  # max downloads per minute\n",
        "        self.snapshot_file = snapshot_file\n",
        "\n",
        "        self.cache = OrderedDict()\n",
        "        self.download_timestamps = deque()\n",
        "        self.load_snapshot()\n",
        "\n",
        "    def save_snapshot(self):\n",
        "        with open(self.snapshot_file, 'wb') as f:\n",
        "            pickle.dump(self.cache, f)\n",
        "\n",
        "    def load_snapshot(self):\n",
        "        if os.path.exists(self.snapshot_file):\n",
        "            with open(self.snapshot_file, 'rb') as f:\n",
        "                self.cache = pickle.load(f)\n",
        "                self.size = sum(item.size for item in self.cache.values())\n",
        "\n",
        "    def _evict(self):\n",
        "\n",
        "        # optimization\n",
        "        if self.eviction_policy == \"LFU\":\n",
        "            while self.lfu_heap:\n",
        "                # insert_time not used but can be used for breaking ties\n",
        "                access_count_at_push, _, url = heappop(self.lfu_heap)\n",
        "                if url in self.cache:\n",
        "                    item = self.cache[url]\n",
        "                    # not stale if access_counts match\n",
        "                    if item.access_count == access_count_at_push:\n",
        "                        del self.cache[url]\n",
        "                        self.size -= item.size\n",
        "                        break  # valid eviction\n",
        "\n",
        "        # practically, this is the same as ordereddict I think\n",
        "        elif self.eviction_policy == \"FIFO\":\n",
        "            while self.fifo_queue:\n",
        "                url = self.fifo_queue.popleft()\n",
        "                # evicted URLs can stay in queue so check this\n",
        "                if url in self.cache:\n",
        "                    item = self.cache.pop(url)\n",
        "                    self.size -= item.size\n",
        "                    break  # evicted\n",
        "\n",
        "        if self.eviction_policy == \"LRU\":\n",
        "            url, item = self.cache.popitem(last=False)\n",
        "        elif self.eviction_policy == \"FIFO\":\n",
        "            url = next(iter(self.cache))\n",
        "            item = self.cache.pop(url)\n",
        "        elif self.eviction_policy == \"LFU\":\n",
        "            url = min(self.cache, key=lambda k: self.cache[k].access_count)\n",
        "            item = self.cache.pop(url)\n",
        "\n",
        "        elif self.eviction_policy == \"PRIORITY\":\n",
        "            url = min(self.cache, key=lambda k: self.cache[k].priority)\n",
        "            item = self.cache.pop(url)\n",
        "        else:\n",
        "            # default\n",
        "            url, item = self.cache.popitem(last=False)\n",
        "\n",
        "        self.size -= item.size\n",
        "\n",
        "    def _is_bandwidth_limited(self):\n",
        "        now = time.time()\n",
        "        # remove timestamps older than 1 minute\n",
        "        if self.download_timestamps:\n",
        "          diff  = now - self.download_timestamps[0]\n",
        "          #print('diff:', diff)\n",
        "\n",
        "        # bandiwth measured per minute so remove any old before calculating numerator\n",
        "        while self.download_timestamps and now - self.download_timestamps[0] > 60:\n",
        "            self.download_timestamps.popleft()\n",
        "\n",
        "        requests_per_minute = len(self.download_timestamps) + 1 # this current\n",
        "        print('requests_per_minute:', requests_per_minute)\n",
        "        return requests_per_minute > self.bandwidth_limit\n",
        "\n",
        "    def _download(self, url):\n",
        "        if self._is_bandwidth_limited():\n",
        "            raise Exception(f\"Bandwidth limit exceeded. Try again later.\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            content = response.content\n",
        "            if len(content) > self.max_item_size:\n",
        "                return f\"Image too large to cache: {url}\"\n",
        "            self.download_timestamps.append(time.time())\n",
        "            return content\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            return f\"Error downloading image: {e}\"\n",
        "\n",
        "    def get(self, url):\n",
        "        #with self.lock:\n",
        "          now = time.time()\n",
        "\n",
        "          # Clean expired items\n",
        "          # Eviction is a separate process than this\n",
        "          expired = [key for key, item in self.cache.items() if now > item.ttl_expiry]\n",
        "          for key in expired:\n",
        "              self.size -= self.cache[key].size\n",
        "              del self.cache[key]\n",
        "\n",
        "          if url in self.cache:\n",
        "              item = self.cache[url]\n",
        "              item.last_access = now\n",
        "              item.access_count += 1\n",
        "              # Only move to end if we're using LRU policy\n",
        "              if self.eviction_policy == \"LRU\":\n",
        "                  self.cache.move_to_end(url)\n",
        "\n",
        "              # Also inside get(), after item.access_count += 1, if LFU, push updated count\n",
        "              if self.eviction_policy == \"LFU\":\n",
        "                  heappush(self.lfu_heap, (item.access_count, item.insert_time, url))\n",
        "\n",
        "              return f\"CACHE HIT: {url}\"\n",
        "\n",
        "          result = self._download(url)\n",
        "\n",
        "          image_size = len(result)\n",
        "          if image_size > self.max_size:\n",
        "              return f\"Image exceeds total cache size: {url}\"\n",
        "\n",
        "          while self.size + image_size > self.max_size:\n",
        "              self._evict()\n",
        "\n",
        "          new_item = CacheItem(url, result, image_size, now)\n",
        "          self.cache[url] = new_item\n",
        "          self.size += image_size\n",
        "\n",
        "          # optimizaiton\n",
        "          if self.eviction_policy == \"LFU\":\n",
        "              heappush(self.lfu_heap, (new_item.access_count + 1, new_item.insert_time, url))\n",
        "          elif self.eviction_policy == \"FIFO\":\n",
        "              self.fifo_queue.append(url)\n",
        "\n",
        "          return f\"DOWNLOADED: {url}\"\n",
        "\n",
        "    def prefetch(self, urls):\n",
        "        for url in urls:\n",
        "            #threading.Thread(target=self.get, args=(url,)).start()\n",
        "            self.get(url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 2:\n",
        "        print(\"Usage: python image_cache.py <input_file>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    input_file = sys.argv[1]\n",
        "\n",
        "    with open(input_file, 'r') as f:\n",
        "        max_cache_size = int(f.readline().strip())\n",
        "        n = int(f.readline().strip())\n",
        "        urls = [f.readline().strip() for _ in range(n)]\n",
        "\n",
        "    cache = ImageCache(max_cache_size, eviction_policy=\"FIFO\")\n",
        "    for url in urls:\n",
        "        print('---')\n",
        "        print(url)\n",
        "        result = cache.get(url)\n",
        "        print(result)\n",
        "        #time.sleep(5)\n",
        "\n",
        "    cache.get(\"https://placehold.co/600x400/EEE/31343C?font=poppins&text=Poppins\")\n",
        "\n",
        "    print('===')\n",
        "    # Example of prefetching (next likely images)\n",
        "    #cache.prefetch([\"https://placehold.co/600x400/EEE/31343C?font=poppins&text=Poppins\"])\n",
        "\n",
        "    # Save snapshot at the end\n",
        "    #cache.save_snapshot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jEtfqXNFjaq",
        "outputId": "341c3cac-c8c0-485d-f6f2-4087b0bb9d06"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting advanced_lru_cache.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile input.txt\n",
        "10000\n",
        "3\n",
        "https://placehold.co/600x400.png\n",
        "https://placehold.co/600x400.png\n",
        "https://placehold.co/150x150.png"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u43yvLlFr0U",
        "outputId": "2dbca283-3aa9-4d2f-f85d-bebab1a3f804"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting input.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python advanced_lru_cache.py input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsMjpIpoFjSq",
        "outputId": "952bf898-ac5c-4be0-ef5a-279b412350f3"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "https://placehold.co/600x400.png\n",
            "requests_per_minute: 1\n",
            "DOWNLOADED: https://placehold.co/600x400.png\n",
            "---\n",
            "https://placehold.co/600x400.png\n",
            "CACHE HIT: https://placehold.co/600x400.png\n",
            "---\n",
            "https://placehold.co/150x150.png\n",
            "requests_per_minute: 2\n",
            "DOWNLOADED: https://placehold.co/150x150.png\n",
            "requests_per_minute: 3\n",
            "===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature                   | Heap-Based LFU                     | `min()` on `OrderedDict` LFU              |\n",
        "| ------------------------- | ---------------------------------- | ----------------------------------------- |\n",
        "| **Eviction Time**         | **O(log n)** (heap push/pop)       | **O(n)** (`min()` over all items)         |\n",
        "| **Access Time (get/put)** | O(1) + heap push = O(log n)        | O(1) + O(1) update + O(n) min at eviction |\n",
        "| **Space Overhead**        | O(n) for `cache`, O(n) for `heap`  | O(n) for `cache` only                     |\n",
        "| **Pros**                  | Fast eviction                      | Simpler implementation                    |\n",
        "| **Cons**                  | More memory and stale heap entries | Slower eviction due to full scan          |\n"
      ],
      "metadata": {
        "id": "zLB51-DPR1F_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature            | Deque-Based FIFO                    | `OrderedDict` FIFO                 |\n",
        "| ------------------ | ----------------------------------- | ---------------------------------- |\n",
        "| **Eviction Time**  | O(1) with stale-check (amortized)   | **O(1)** via `popitem(last=False)` |\n",
        "| **Access Time**    | O(1) + stale cleanup                | **O(1)**                           |\n",
        "| **Space Overhead** | O(n) for `cache` + O(n) for `deque` | O(n) for single `OrderedDict`      |\n",
        "| **Pros**           | Explicit queue logic, flexible      | Cleanest and most efficient        |\n",
        "| **Cons**           | Potential stale entries             | Less customizable                  |\n"
      ],
      "metadata": {
        "id": "SWF_XDNWR3uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Policy   | Best for Performance | Best for Simplicity |\n",
        "| -------- | -------------------- | ------------------- |\n",
        "| **LFU**  | Heap-based           | `min()` + `dict`    |\n",
        "| **FIFO** | `OrderedDict`        | `OrderedDict`       |\n"
      ],
      "metadata": {
        "id": "qLw2ONHAR5Dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Strategy**                         | **Pros**                                                                 | **Cons**                                                       | **Use Cases**                                                                    |\n",
        "| ------------------------------------ | ------------------------------------------------------------------------ | -------------------------------------------------------------- | -------------------------------------------------------------------------------- |\n",
        "| **LRU (Least Recently Used)**        | Simple and efficient eviction of least recently accessed items           | Poor for workloads with frequent full scans or cyclic patterns | Browser caches, database buffers, in-memory caches (e.g., `functools.lru_cache`) |\n",
        "| **LFU (Least Frequently Used)**      | Retains hot items accessed repeatedly, good for skewed access patterns   | Requires frequency tracking; may retain stale \"popular\" items  | CDN edge caches, recommendation systems                                          |\n",
        "| **FIFO (First In First Out)**        | Simple and predictable                                                   | Ignores access pattern; may evict useful items                 | Streaming data, write-behind caches                                              |\n",
        "| **Random Replacement**               | Low-overhead eviction                                                    | Non-deterministic; can evict hot items                         | Hardware caches (e.g., CPU), when memory is tight                                |\n",
        "| **TTL (Time to Live)**               | Auto-purges stale items after a time threshold                           | Fixed expiration may evict still-useful items                  | DNS caching, session tokens                                                      |\n",
        "| **ARC (Adaptive Replacement Cache)** | Balances between LRU and LFU adaptively                                  | Complex to implement                                           | Filesystem page caching (ZFS, IBM DB2)                                           |\n",
        "| **MRU (Most Recently Used)**         | Good for patterns where recently used items are less likely to be reused | Counterintuitive for many apps; rarely used alone              | Specific DB workloads (e.g., full table scans), specialized memory pools         |\n",
        "| **2Q Cache**                         | Improves LRU by filtering out short-lived entries                        | Slightly more complex than LRU                                 | Web servers, virtual memory systems                                              |\n",
        "| **Segmented LRU**                    | Prevents cache pollution from one workload segment                       | Needs workload classification                                  | JVM memory pools, multi-tenant environments                                      |\n"
      ],
      "metadata": {
        "id": "XbvYuwlHSnV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile word_counter.py\n",
        "\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "\n",
        "def word_count(file_path):\n",
        "    counts = defaultdict(int)\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            for word in line.strip().split():\n",
        "                counts[word] += 1\n",
        "\n",
        "    for word in sorted(counts):\n",
        "        print(f\"{word} {counts[word]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) < 2:\n",
        "        print(\"Usage: python word_counter.py <input_file>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    word_count(sys.argv[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2ILT-H-_058",
        "outputId": "52f5728e-3fe5-4e8f-8c6e-8c718c3dd7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing word_counter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile input.txt\n",
        "apple banana apple\n",
        "orange apple banana"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqi2tDvPCMct",
        "outputId": "9e8cbb05-5954-429b-c818-3651602f5e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing input.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python word_counter.py input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ryWpHHjCHw_",
        "outputId": "8f5155cd-9d0b-4532-d017-741bf5fe23d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apple 3\n",
            "banana 2\n",
            "orange 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a short snippet that writes into csv file and then reads it back\n",
        "\n",
        "import csv\n",
        "\n",
        "def write_and_read_csv(filename, data):\n",
        "    # Write data to CSV file\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerows(data)\n",
        "\n",
        "def csv_reader(csvfile):\n",
        "    # Read data from CSV file\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        read_data = list(reader)\n",
        "\n",
        "    return read_data\n",
        "\n",
        "\n",
        "# Example usage\n",
        "data = [\n",
        "    [\"Name\", \"Age\", \"City\"],\n",
        "    [\"Alice\", \"25\", \"New York\"],\n",
        "    [\"Bob\", \"30\", \"Los Angeles\"],\n",
        "    [\"Charlie\", \"28\", \"Chicago\"]\n",
        "]\n",
        "\n",
        "filename = \"example.csv\"\n",
        "read_data = write_and_read_csv(filename, data)\n",
        "print(csv_reader(filename))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlimylDmCaIm",
        "outputId": "1f84ef0d-0097-4e7c-c1a1-8f95fa68ad1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Name', 'Age', 'City'], ['Alice', '25', 'New York'], ['Bob', '30', 'Los Angeles'], ['Charlie', '28', 'Chicago']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fWTWRRQ8DB67"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}